{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa librería encargada de cargar los procesos para imágenes\n",
    "import cv2\n",
    "# Se importa numpy para el manejo de array\n",
    "import numpy as np \n",
    "# Se importa os para la lectura de directorio de imágenes\n",
    "from os import listdir\n",
    "# Se importa pyplot para mostrar imágenes y gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "# Se importa pandas para trabajar con csv\n",
    "import pandas as pd\n",
    "# Se importa math para funciones\n",
    "import math as mt\n",
    "# Se importa joblib para poder cargar y guardar modelos\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Se importa skimage feature para la extración de características de GLCM\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "\n",
    "# Se importan medidas de desempeño y error\n",
    "from sklearn import metrics\n",
    "# Se importan Repeated K-Fold Cross Validator para generar los conjuntos de Entrenamiento y Test \n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "# Se importa la función para el calculo de la precisión\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Se importa la función para obtener la matriz de confusión\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Se importa el divisor de data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Se importa el modelo de clasificación correspondiente a una SVM en este caso SVC (C-support vector classification)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Se importa StandardScaler para poder estandarizar los valores de los features para tener la misma escala \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Se importa ssim para poder calcular la diferencia entre imágenes\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "\n",
    "# Se importa para poder calcular medidas de tendencia centrales\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(100)\n",
    "\n",
    "class BatAlgorithm():\n",
    "    def __init__(self, D, NB, N_Gen, A0, r0, alpha, gamma, Fmin, Fmax, LowerB, UpperB, function, imgSamples, dataSamples):\n",
    "        \n",
    "        # Inicialización de Parámetros\n",
    "        self.D = D  #dimension\n",
    "        self.NB = NB  #number of bats \n",
    "        self.N_Gen = N_Gen  #generations\n",
    "        self.alpha = alpha  # loudness update\n",
    "        self.gamma = gamma  # pulse rate update\n",
    "        self.r0 = r0 # Pulse rate initial\n",
    "        self.A0 = A0 # Loudnes initial\n",
    "        self.Fmin = Fmin  #frequency min\n",
    "        self.Fmax = Fmax  #frequency max\n",
    "        self.LowerB = LowerB  #lower bound\n",
    "        self.UpperB = UpperB  #upper bound\n",
    "        self.Funct = function #function to optimize\n",
    "        \n",
    "        # Data Training\n",
    "        self.imgSamples = imgSamples\n",
    "        self.dataSamples = dataSamples\n",
    "        \n",
    "        # Inicializar Loudness (A) y Pulse rate (R)\n",
    "        self.A = [self.A0 for i in range(self.NB)]\n",
    "        self.r = [self.r0 for i in range(self.NB)]\n",
    "        \n",
    "        # Inicializar limite inferior y superior para cada bat\n",
    "        self.Lb = [[0.0 for i in range(self.D)] for j in range(self.NB)]  #lower bound\n",
    "        self.Ub = [[0.0 for i in range(self.D)] for j in range(self.NB)]  #upper bound\n",
    "        \n",
    "        # Inicializar frecuencia\n",
    "        self.F = [0.0] * self.NB  #frequency\n",
    "\n",
    "        # Inicializar velocidad de los bats\n",
    "        self.v = [[0.0 for i in range(self.D)] for j in range(self.NB)]  #velocity\n",
    "        \n",
    "        # Inicilizar soluciones de los bats\n",
    "        self.Sol = [[0.0 for i in range(self.D)] for j in range(self.NB)]  #population of solutions\n",
    "        \n",
    "        # Inicilizar fitness de cada bat y el mejor fitness\n",
    "        self.Fitness = [0.0] * self.NB  #fitness\n",
    "        self.f_max = 0.0  #maximum fitness\n",
    "        \n",
    "        # Se agregan las variables de matriz de confusión\n",
    "        self.confM = [0.0] * self.NB #matriz de confusión\n",
    "        self.maxconfM = 0 #máxima matriz de confusión\n",
    "        \n",
    "        # Inicilizar la mejor solución encontrada\n",
    "        self.best = [0.0] * self.D  #best solution\n",
    "        \n",
    "        \n",
    "    def best_bat(self):\n",
    "        i = 0\n",
    "        j = 0\n",
    "        # Se obtiene el mejor bat de toda la 1ra generación\n",
    "        for i in range(self.NB):\n",
    "            if self.Fitness[i] > self.Fitness[j]: # Cambiar para max o min\n",
    "                j = i\n",
    "        # Se actualiza el fitness mejor y la sol para la 1ra vez\n",
    "        for i in range(self.D):\n",
    "            self.best[i] = self.Sol[j][i]\n",
    "        self.f_max = self.Fitness[j]\n",
    "        self.maxconfM = self.confM[j]\n",
    "\n",
    "    def init_bat(self):\n",
    "        # Iniciliza los limites para cada bat y variables\n",
    "        for i in range(self.NB):\n",
    "            for j in range(self.D):\n",
    "                self.Lb[i][j] = self.LowerB[j]\n",
    "                self.Ub[i][j] = self.UpperB[j]\n",
    "                \n",
    "        # Se inicializan las soluciones para cada bat\n",
    "        for i in range(self.NB):\n",
    "            self.F[i] = 0\n",
    "            for j in range(self.D):\n",
    "                rnd = np.random.uniform(0, 1)\n",
    "                self.v[i][j] = 0.0\n",
    "                self.Sol[i][j] = self.Lb[i][j] + (self.Ub[i][j] - self.Lb[i][j]) * rnd\n",
    "            self.Fitness[i], self.confM[i] = self.Funct(self.D, self.Sol[i], self.imgSamples, self.dataSamples)\n",
    "        \n",
    "        # Se selecciona el mejor bat de la generacion?\n",
    "        self.best_bat()\n",
    "\n",
    "    def simplebounds(self, val,index):\n",
    "        # Función encargada de normalizar las soluciones en los limites\n",
    "        if(index == 0):\n",
    "            if val > self.UpperB[index]:\n",
    "                val = self.UpperB[index]\n",
    "            if val < self.LowerB[index]:\n",
    "                val = self.LowerB[index]\n",
    "        if(index == 1):\n",
    "            if val > self.UpperB[index]:\n",
    "                val = self.UpperB[index]\n",
    "            if val < self.LowerB[index]:\n",
    "                val = self.LowerB[index]\n",
    "    \n",
    "        return val\n",
    "\n",
    "    def move_bat(self):\n",
    "        # Se inicializa matriz de soluciones \n",
    "        S = [[0.0 for i in range(self.D)] for j in range(self.NB)]\n",
    "\n",
    "        self.init_bat()\n",
    "\n",
    "        for t in range(self.N_Gen):\n",
    "            factor = np.mean(self.A)\n",
    "            for i in range(self.NB):\n",
    "                rnd = np.random.uniform(0, 1)\n",
    "                self.F[i] = self.Fmin + (self.Fmax - self.Fmin) * rnd\n",
    "                \n",
    "                for j in range(self.D):\n",
    "                    self.v[i][j] = self.v[i][j] + (self.best[j] - self.Sol[i][j]) * self.F[i]\n",
    "                    S[i][j] = self.Sol[i][j] + self.v[i][j]\n",
    "\n",
    "                    S[i][j] = self.simplebounds(S[i][j],j)\n",
    "\n",
    "                rnd = np.random.uniform(0,1)\n",
    "\n",
    "                if rnd > self.r[i]:\n",
    "                    for j in range(self.D):\n",
    "                        rnd = np.random.uniform(-1.0,1.0)\n",
    "                        S[i][j] = self.best[j] + rnd*factor\n",
    "                        S[i][j] = self.simplebounds(S[i][j],j)\n",
    "                        \n",
    "                Fnew, Mconf = self.Funct(self.D, S[i], self.imgSamples, self.dataSamples)\n",
    "\n",
    "                rnd = np.random.uniform(0,1)\n",
    "\n",
    "                if (Fnew > self.Fitness[i]) and (rnd < self.A[i]): # Cambiar para max o min\n",
    "                    for j in range(self.D):\n",
    "                        self.Sol[i][j] = S[i][j]\n",
    "                    self.Fitness[i] = Fnew\n",
    "                    self.confM[i] = Mconf\n",
    "\n",
    "                if Fnew > self.f_max: # Cambiar para max o min\n",
    "                    for j in range(self.D):\n",
    "                        self.best[j] = self.Sol[i][j]\n",
    "                    self.f_max = Fnew\n",
    "                    self.maxconfM = Mconf\n",
    "                    \n",
    "                    self.A[i] = self.A[i]*self.alpha\n",
    "                    self.r[i] = self.r0*(1 - mt.exp(-1*self.gamma*i))\n",
    "            print(\"\\nFitness de generación:\",self.Fitness)\n",
    "            print(\"Fitness mejor encontrado:\",self.f_max)\n",
    "            #print(\"Matriz de Confusión:\\n\",self.confM)\n",
    "            print(\"Solucion:\",self.best)\n",
    "                    \n",
    "        print (\"\\nFitness máximo: \",self.f_max)\n",
    "        print (\"Solución para máximo: \",self.best)\n",
    "        print (\"Matriz de Confusión: \",self.maxconfM)\n",
    "        return self.best, self.maxconfM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función encargada de cargar las imágenes y obtener sus features\n",
    "def cargarFotosYFeatures(path):\n",
    "    casos = listdir(path)\n",
    "    featureList = []\n",
    "    contadorCaso = 0\n",
    "    \n",
    "    #------------- SECCIÓN ENCARGADA DE CREAR LAS IMÁGENES A COMPARAR EN LA SSIM -------------#\n",
    "    imgNueva = np.zeros((75,100))\n",
    "    imgNueva2 = np.zeros((75,100))\n",
    "    lineThickness = 1\n",
    "    cv2.line(imgNueva, (0, 75), (100, 0), (255,255,255), lineThickness)\n",
    "    cv2.line(imgNueva2, (0, 75), (100, 45), (255,255,255), lineThickness)\n",
    "    #------------- SECCIÓN ENCARGADA DE CREAR LAS IMÁGENES A COMPARAR EN LA SSIM -------------#\n",
    "    \n",
    "    # Se cargan las imágenes y se extraen sus features\n",
    "    contador = 1\n",
    "    for caso in casos:\n",
    "        nombreImgs = listdir(path+caso+\"/\")\n",
    "        featureCaso = []\n",
    "        for nombre in nombreImgs:\n",
    "            if(nombre != \"Thumbs.db\"):\n",
    "                img = cv2.imread(path+caso+\"/\"+nombre)\n",
    "                img = getROIyFeatures(img,imgNueva,imgNueva2)\n",
    "                featureCaso.append(img)\n",
    "                contador = contador + 1\n",
    "        featureList.append(featureCaso)\n",
    "        contadorCaso = contadorCaso +1\n",
    "        if(contadorCaso%50 == 0):\n",
    "            print(\"Cargadas fotografías hasta caso: \"+caso)\n",
    "    print(\"Fotografías cargadas: \"+str(contador))\n",
    "    return featureList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función encargada de obtener ROI y features de estos\n",
    "def getROIyFeatures(img,imgNueva,imgNueva2):\n",
    "    # Se obtiene las 3 ROI a analizar\n",
    "    imgROI1 = img[0:36,50:100] \n",
    "    imgROI2 = img[36:75,0:50] \n",
    "    imgROI3 = img[36:75,50:100] \n",
    "    \n",
    "    # Se obtienen las features de cada ROI\n",
    "    features1 = np.array(getFeatures(imgROI1))\n",
    "    features2 = np.array(getFeatures(imgROI2))\n",
    "    features3 = np.array(getFeatures(imgROI3))\n",
    "    \n",
    "    # Se obtienen las features de la imagen para SSIM y MSE\n",
    "    features4 = np.array(getSSIMyMSE(img,imgNueva,imgNueva2))\n",
    "\n",
    "    # Se concatenan todas en un arreglo de 32 features\n",
    "    features = np.concatenate((features1,features2,features3,features4),axis= None)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función encargada de obtener los features por imagen\n",
    "def getFeatures(img):\n",
    "    features = []\n",
    "    \n",
    "    #------------------ HISTOGRAM ----------------------#\n",
    "    # Features de histograma\n",
    "    hist = histogram(img)\n",
    "    \n",
    "    mean = np.mean(hist)\n",
    "    std = np.std(hist) \n",
    "    \n",
    "    features.append(mean)\n",
    "    features.append(std)\n",
    "    \n",
    "    #------------------ HISTOGRAM ----------------------#\n",
    "    \n",
    "    img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    #------------------ GLCM ---------------------------#\n",
    "    \n",
    "    # Features de GLCM\n",
    "    glcm = greycomatrix(img, [1], [0], 256, symmetric=True, normed=True)\n",
    "    dissimilarity = greycoprops(glcm, 'dissimilarity')\n",
    "    contrast = greycoprops(glcm, 'contrast')\n",
    "    homogeneity = greycoprops(glcm, 'homogeneity')\n",
    "    ASM = greycoprops(glcm, 'ASM')\n",
    "    energy = greycoprops(glcm, 'energy')\n",
    "    correlation = greycoprops(glcm, 'correlation')\n",
    "    \n",
    "    # Características en 0 grados\n",
    "    features.append(float(dissimilarity))\n",
    "    features.append(float(contrast))\n",
    "    features.append(float(homogeneity))\n",
    "    features.append(float(ASM))\n",
    "    features.append(float(energy))\n",
    "    features.append(float(correlation))\n",
    "    \n",
    "    #------------------ GLCM ---------------------------#\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función encargada de calcular el histograma de colores de una imagen\n",
    "def histogram(img):\n",
    "    color = ('b','g','r')\n",
    "    for i, c in enumerate(color):\n",
    "        hist = cv2.calcHist([img], [i], None, [256], [0, 256])\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función encargada de calcular el error cuadrático medio entre 2 imágenes\n",
    "def mse(imageA, imageB):\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función encargada de calcular el índice de similaridad estructural\n",
    "def getSSIMyMSE(img,imgNueva,imgNueva2):\n",
    "    img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(img,100,200,apertureSize = 5)\n",
    "    \n",
    "    # Se aplica una máscara para obtener la 1ra sección a comparar por SSIM y MSE\n",
    "    mask = np.zeros(edges.shape, dtype=np.uint8)\n",
    "    roi_corners = np.array([[(90,10),(0,75),(90,0)]], dtype=np.int32)\n",
    "    ignore_mask_color = (255,)\n",
    "    cv2.fillPoly(mask, roi_corners, ignore_mask_color)\n",
    "    masked_image = cv2.bitwise_and(edges, mask)\n",
    "    \n",
    "    # Se aplica una máscara para obtener la 2da sección a comparar por SSIM y MSE\n",
    "    mask2 = np.zeros(edges.shape, dtype=np.uint8)\n",
    "    roi_corners2 = np.array([[(90,60),(0,75),(90,40)]], dtype=np.int32)\n",
    "    ignore_mask_color2 = (255,)\n",
    "    cv2.fillPoly(mask2, roi_corners2, ignore_mask_color2)\n",
    "    masked_image2 = cv2.bitwise_and(edges, mask2)\n",
    "    \n",
    "    # Se compara cada sección con la correspondiente recta para obtener las features\n",
    "    ssim_const = ssim(masked_image,imgNueva,data_range=imgNueva.max() - imgNueva.min())\n",
    "    MSE = mse(masked_image,imgNueva)\n",
    "    \n",
    "    ssim_const2 = ssim(masked_image,imgNueva2,data_range=imgNueva.max() - imgNueva.min())\n",
    "    MSE2 = mse(masked_image,imgNueva2) \n",
    "    \n",
    "    ssim_const3 = ssim(masked_image2,imgNueva,data_range=imgNueva.max() - imgNueva.min())\n",
    "    MSE3 = mse(masked_image2,imgNueva)\n",
    "    \n",
    "    ssim_const4 = ssim(masked_image2,imgNueva2,data_range=imgNueva.max() - imgNueva.min())\n",
    "    MSE4 = mse(masked_image2,imgNueva2)\n",
    "    \n",
    "    # Se agrega cada feature a un arreglo a retornar\n",
    "    features = []\n",
    "    features.append(ssim_const)\n",
    "    features.append(ssim_const2) \n",
    "    features.append(ssim_const3) \n",
    "    features.append(ssim_const4)\n",
    "    features.append(MSE)\n",
    "    features.append(MSE2) \n",
    "    features.append(MSE3) \n",
    "    features.append(MSE4)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se la información de los labels de cada imagen\n",
    "dataY = pd.read_csv(\"images_250/GRIGRI_Training_250.csv\",header=None)\n",
    "dataY = pd.Series.ravel(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargadas fotografías hasta caso: 071\n",
      "Cargadas fotografías hasta caso: 125\n",
      "Cargadas fotografías hasta caso: 182\n",
      "Cargadas fotografías hasta caso: 241\n",
      "Cargadas fotografías hasta caso: 301\n",
      "Fotografías cargadas: 5001\n"
     ]
    }
   ],
   "source": [
    "# Se cargan las fotografías y se obtienen sus features\n",
    "featuresImgs = cargarFotosYFeatures(\"images_250/dirTraining/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se preparan los datos y labels para utilizarlos en la SVM\n",
    "imgSamples = []\n",
    "dataSamples = []\n",
    "for featureArray,label in zip(featuresImgs,dataY):\n",
    "    for featuresImg in featureArray:\n",
    "        imgSamples.append(featuresImg)\n",
    "        dataSamples.append(label)\n",
    "imgSamples = np.array(imgSamples)\n",
    "dataSamples = np.array(dataSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.03125000e+00 9.94633026e+01 1.67517007e+01 3.03686508e+03\n",
      " 8.52264619e-01 7.19409447e-01 8.48180080e-01 5.96968212e-01\n",
      " 7.61718750e+00 1.00648178e+02 3.05306122e+01 5.48792360e+03\n",
      " 7.54851656e-01 5.61961362e-01 7.49640822e-01 4.69728669e-01\n",
      " 7.61718750e+00 9.75972748e+01 2.74296180e+01 4.85718786e+03\n",
      " 7.42062090e-01 5.35533354e-01 7.31801444e-01 5.98688534e-01\n",
      " 8.28640987e-01 7.33960115e-01 6.51166252e-01 7.94100696e-01\n",
      " 1.59528000e+03 1.89006000e+03 3.26859000e+03 2.78307000e+03]\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(imgSamples[0])\n",
    "print(dataSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.03125000e+00 9.94633026e+01 1.67517007e+01 3.03686508e+03\n",
      " 8.52264619e-01 7.19409447e-01 8.48180080e-01 5.96968212e-01\n",
      " 7.61718750e+00 1.00648178e+02 3.05306122e+01 5.48792360e+03\n",
      " 7.54851656e-01 5.61961362e-01 7.49640822e-01 4.69728669e-01\n",
      " 7.61718750e+00 9.75972748e+01 2.74296180e+01 4.85718786e+03\n",
      " 7.42062090e-01 5.35533354e-01 7.31801444e-01 5.98688534e-01\n",
      " 8.28640987e-01 7.33960115e-01 6.51166252e-01 7.94100696e-01\n",
      " 1.59528000e+03 1.89006000e+03 3.26859000e+03 2.78307000e+03]\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(imgSamples[0])\n",
    "print(dataSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# Se revisa la estructura de los datos previo al entrenamiento\n",
    "print(imgSamples.shape)\n",
    "print(dataSamples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "(3200, 32)\n",
      "(3200,)\n",
      "Validación:\n",
      "(800, 32)\n",
      "(800,)\n",
      "Testing:\n",
      "(1000, 32)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# Se separan los datos en 80% y 20%\n",
    "imgSamplesTrainF, imgSamplesTest, dataSamplesTrainF, dataSamplesTest = train_test_split(imgSamples, dataSamples, \n",
    "                                                                                      test_size=0.20, random_state=0)\n",
    "# Se separan los datos de training en training y validation 90% y 10% \n",
    "imgSamplesTrain, validationSet, dataSamplesTrain, dataValidationSet = train_test_split(imgSamplesTrainF, dataSamplesTrainF, \n",
    "                                                                                      test_size=0.20, random_state=0)\n",
    "\n",
    "print(\"Training:\")\n",
    "print(imgSamplesTrain.shape)\n",
    "print(dataSamplesTrain.shape)\n",
    "print(\"Validación:\")\n",
    "print(validationSet.shape)\n",
    "print(dataValidationSet.shape)\n",
    "print(\"Testing:\")\n",
    "print(imgSamplesTest.shape)\n",
    "print(dataSamplesTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función encargada de entrenar un modelo de SVM (esta función es utilizada por el algoritmo de optimización en un Bat)\n",
    "def optimizeModel(D,sol,imgSamples,dataSamples):\n",
    "    # Se generan arreglos para guardar las métricas para el modelo a entrenar\n",
    "    accuracyClasificacion = []\n",
    "    confusionM = 0\n",
    "    # Se generan los conjuntos para 5 Folds con 10 iteraciones del proceso y random_state fijado para obtener los mismos resultados\n",
    "    RepKFoldVal = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n",
    "\n",
    "    # Se realiza una iteración para generar los conjuntos de entrenamiento y test luego del Repeated K-Fold Cross Validation\n",
    "    for indice_train, indice_test in RepKFoldVal.split(imgSamples,dataSamples):\n",
    "        # Se obtiene los arreglos con los conjuntos de entrenamiento y test\n",
    "        X_train, X_test = imgSamples[indice_train], imgSamples[indice_test]\n",
    "        Y_train, Y_test = dataSamples[indice_train], dataSamples[indice_test]\n",
    "\n",
    "        # Se escalan los datos\n",
    "        sc = StandardScaler()\n",
    "        sc.fit(X_train)\n",
    "        X_train = sc.transform(X_train)\n",
    "        X_test  = sc.transform(X_test)        \n",
    "        \n",
    "        # Se instancia un clasificador de tipo SVM\n",
    "        svmClass = SVC(gamma=sol[0],kernel='rbf',C=sol[1])\n",
    "        svmClass.fit(X_train,Y_train)\n",
    "\n",
    "        # Se obtienen las predicciones del modelo SVM\n",
    "        predictSVM = svmClass.predict(X_test)\n",
    "        exactitud = svmClass.score(X_test,Y_test)\n",
    "        \n",
    "        # Se obtiene la matriz de confusión del modelo\n",
    "        confusionM = confusionM + confusion_matrix(Y_test, predictSVM)\n",
    "        \n",
    "        # Se obtiene la precisión del modelo SVM para esta iteración\n",
    "        accuracyClasificacion.append(exactitud)\n",
    "        \n",
    "    return np.mean(accuracyClasificacion), confusionM/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se instancian los parámetros del Bat y se realiza el proceso de optimización\n",
    "ba = BatAlgorithm(2,10,20,0.95,0.1,0.95,0.95,0,1,[1/32, 4],[1, 8],optimizeModel,validationSet,dataValidationSet)\n",
    "parametros, matriz  = ba.move_bat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se escalan los datos\n",
    "sc = StandardScaler()\n",
    "sc.fit(imgSamplesTrain)\n",
    "imgSamplesTrainScaled = sc.transform(imgSamplesTrain)\n",
    "imgSamplesTestScaled  = sc.transform(imgSamplesTest)\n",
    "\n",
    "# Entrenar modelo definitivo\n",
    "clfClass = SVC(gamma=parametros[0],kernel='rbf',C=parametros[1])\n",
    "clfClass.fit(imgSamplesTrainScaled,dataSamplesTrain)\n",
    "\n",
    "# Se obtiene la exactitud y la matriz de confusión\n",
    "exactitud = clfClass.score(imgSamplesTestScaled,dataSamplesTest)\n",
    "predictClf = clfClass.predict(imgSamplesTestScaled)\n",
    "confusionM = confusion_matrix(dataSamplesTest, predictClf)\n",
    "\n",
    "print(\"Exactitud: \"+str(exactitud))\n",
    "print(\"Matriz de Confusión: \"+str(confusionM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
